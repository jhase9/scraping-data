{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Journalism Job Searching\n",
    "This project scrapes data of recent job postings from several journalism job posting websites and aggregates them into a dataframe. \n",
    "\n",
    "It then provides search functionality by keyword and location, allows the user to open links to job postings from specified organizations, and gives and overview of where jobs are located.\n",
    "\n",
    "<i>Before starting, run the source code below. Then, run any/all functions.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key:\n",
    "\n",
    "- Type <strong>find_all_jobs()</strong> to see a dataframe of recent job postings from main journalism job posting sites. <i>(You can also type in <strong>find_ONA_jobs(), find_JJ_jobs(), or find_IRE_jobs()</strong> to find postings from specific websites.)</i>\n",
    "\n",
    "- Type <strong>where_are_jobs()</strong> to see a chart of jobs by location\n",
    "\n",
    "- Type <strong>jobs_in()</strong> to find a job by location\n",
    "\n",
    "- Type  <strong>search_jobtitles()</strong> to find a job by keyword\n",
    "\n",
    "- Type <strong>see_job_site()</strong> to open webpages from an organization's job postings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#See data of recent job postings from main journalism job posting sites\n",
    "find_all_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Chart jobs by location\n",
    "where_are_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Search jobs by location\n",
    "jobs_in()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Search jobs by keyword\n",
    "search_jobtitles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Open webpages of organization's job postings\n",
    "see_job_site()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Run this first ######\n",
    "\n",
    "#import necessary items\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "import webbrowser\n",
    "\n",
    "# Finding jobs on the Online News Association (ONA) website \n",
    "# Wish we could scrape all sites with one function, but since classes and tags differ it's more difficult to do\n",
    "def find_ONA_jobs():\n",
    "    source=urllib.request.urlopen('https://careers.journalists.org/jobs/').read()\n",
    "    ONA=BeautifulSoup(source, 'html.parser')\n",
    "    soup1 = ONA.find_all('div', {'class' : 'bti-ui-job-result-detail-title'})\n",
    "    y=0\n",
    "    titles = []\n",
    "    links = []\n",
    "    for x in soup1:\n",
    "        yum = x.find_all('a')\n",
    "        for a in yum:\n",
    "            titles.append(a['title'])\n",
    "            links.append(\"https://careers.journalists.org\"+a['href'])\n",
    "            y+= 1\n",
    "\n",
    "        \n",
    "    date_posted = ONA.find_all('div', {'class' : 'bti-ui-job-result-detail-age'}) \n",
    "    dates = []\n",
    "    for x in date_posted:\n",
    "        x = x.get_text().strip()\n",
    "        dates.append(x)\n",
    "    \n",
    "    organization = ONA.find_all('div', {'class' : 'bti-ui-job-result-detail-employer'}) \n",
    "    organizations = []\n",
    "    for x in organization:\n",
    "        x = x.get_text().strip()\n",
    "        organizations.append(x)\n",
    "    \n",
    "    location = ONA.find_all('div', {'class' : 'bti-ui-job-result-detail-location'}) \n",
    "    locations = []\n",
    "    for x in location:\n",
    "        x = x.get_text().strip()\n",
    "        locations.append(x)\n",
    "    \n",
    "    \n",
    "    ONA_jobs = pd.DataFrame({'Job Title': titles, 'Organization': organizations, 'Link' : links, 'Date Posted': dates, 'Location' : locations})\n",
    "    \n",
    "    return ONA_jobs\n",
    "\n",
    "\n",
    "# Finding jobs on the JournalismJobs.com website \n",
    "def find_JJ_jobs():\n",
    "\n",
    "    source3=urllib.request.urlopen('http://www.journalismjobs.com/job-listings?keywords=&location=').read()\n",
    "    JJ=BeautifulSoup(source3, 'html.parser')\n",
    "\n",
    "    soup3 = JJ.find_all('div', {'class' : 'result'})\n",
    "    soup4 = JJ.find_all('div', {'class' : 'title'})\n",
    "    y=0\n",
    "    titles3 = []\n",
    "    links3 = []\n",
    "    for t in soup3:\n",
    "        for x in soup4: \n",
    "            z = x.get_text().strip()\n",
    "            titles3.append(z)\n",
    "            yum3 = x.find_all('a')\n",
    "            for a in yum3:\n",
    "                links3.append(\"http://www.journalismjobs.com\"+a['href'])\n",
    "                y+= 1\n",
    " \n",
    "    date_posted3 = JJ.find_all('li', {'class' : 'posted'})\n",
    "    dates3 = []\n",
    "    for t in soup3:\n",
    "        for x in date_posted3:\n",
    "            x = x.get_text().strip()\n",
    "            dates3.append(x)\n",
    "    \n",
    "    organization3 = JJ.find_all('div', {'class' : 'company'}) \n",
    "    organizations3 = []\n",
    "    for t in soup3:\n",
    "        for x in organization3:\n",
    "            x = x.get_text().strip()\n",
    "            organizations3.append(x)\n",
    "\n",
    "    location3 = JJ.find_all('li', {'class' : 'location'}) \n",
    "    locations3 = []\n",
    "    for t in soup3:\n",
    "        for x in location3:\n",
    "            x = x.get_text().strip()\n",
    "            locations3.append(x)  \n",
    "        \n",
    "    JJ_jobs = pd.DataFrame({'Job Title': titles3, 'Organization': organizations3, 'Link' : links3, 'Date Posted': dates3, 'Location' : locations3})\n",
    "    JJ_jobs = JJ_jobs.drop_duplicates()\n",
    "    return JJ_jobs\n",
    "\n",
    "\n",
    "#Finding jobs on the Investigative Reporters & Editors (IRE) website\n",
    "def find_IRE_jobs():\n",
    "\n",
    "    source2=urllib.request.urlopen('https://www.ire.org/jobs/type/job-posting/').read()\n",
    "    IRE=BeautifulSoup(source2, 'html.parser')\n",
    "\n",
    "\n",
    "    soup2 = IRE.find_all('td', {'class' : 'title3'})\n",
    "    y2=0\n",
    "    links2 = []\n",
    "    for x in soup2:\n",
    "        yum2 = x.find_all('a')\n",
    "        for a in yum2:\n",
    "            links2.append(\"https://www.ire.org\"+a['href'])\n",
    "            y2+= 1\n",
    "\n",
    "    IRE_jobs = pd.read_html(\"https://www.ire.org/jobs/type/job-posting/\", header=None)[0]\n",
    "    IRE_jobs.columns = [\"Job Title\", \"Organization\", \"Location\", \"Date Posted\"]\n",
    "\n",
    "    links2 = pd.Series(links2)\n",
    "\n",
    "    IRE_jobs['Link'] = links2.values\n",
    "    \n",
    "    return IRE_jobs\n",
    "\n",
    "\n",
    "#Putting jobs from each site together, plus standardizing location formatting\n",
    "def find_all_jobs():\n",
    "\n",
    "    job_sources = [find_ONA_jobs(), find_IRE_jobs(), find_JJ_jobs()]\n",
    "\n",
    "    all_jobs = pd.concat(job_sources)\n",
    "\n",
    "    all_jobs['City'], all_jobs['State'] = all_jobs['Location'].str.split(', ', 1).str\n",
    "    \n",
    "    state_abvns = {\n",
    "        'AK': 'Alaska',\n",
    "        'AL': 'Alabama',\n",
    "        'AR': 'Arkansas',\n",
    "        'AS': 'American Samoa',\n",
    "        'AZ': 'Arizona',\n",
    "        'CA': 'California',\n",
    "        'CO': 'Colorado',\n",
    "        'CT': 'Connecticut',\n",
    "        'DC': 'District of Columbia',\n",
    "        'DE': 'Delaware',\n",
    "        'FL': 'Florida',\n",
    "        'GA': 'Georgia',\n",
    "        'GU': 'Guam',\n",
    "        'HI': 'Hawaii',\n",
    "        'IA': 'Iowa',\n",
    "        'ID': 'Idaho',\n",
    "        'IL': 'Illinois',\n",
    "        'IN': 'Indiana',\n",
    "        'KS': 'Kansas',\n",
    "        'KY': 'Kentucky',\n",
    "        'LA': 'Louisiana',\n",
    "        'MA': 'Massachusetts',\n",
    "        'MD': 'Maryland',\n",
    "        'ME': 'Maine',\n",
    "        'MI': 'Michigan',\n",
    "        'MN': 'Minnesota',\n",
    "        'MO': 'Missouri',\n",
    "        'MP': 'Northern Mariana Islands',\n",
    "        'MS': 'Mississippi',\n",
    "        'MT': 'Montana',\n",
    "        'NA': 'National',\n",
    "        'NC': 'North Carolina',\n",
    "        'ND': 'North Dakota',\n",
    "        'NE': 'Nebraska',\n",
    "        'NH': 'New Hampshire',\n",
    "        'NJ': 'New Jersey',\n",
    "        'NM': 'New Mexico',\n",
    "        'NV': 'Nevada',\n",
    "        'NY': 'New York',\n",
    "        'OH': 'Ohio',\n",
    "        'OK': 'Oklahoma',\n",
    "        'OR': 'Oregon',\n",
    "        'PA': 'Pennsylvania',\n",
    "        'PR': 'Puerto Rico',\n",
    "        'RI': 'Rhode Island',\n",
    "        'SC': 'South Carolina',\n",
    "        'SD': 'South Dakota',\n",
    "        'TN': 'Tennessee',\n",
    "        'TX': 'Texas',\n",
    "        'UT': 'Utah',\n",
    "        'VA': 'Virginia',\n",
    "        'VI': 'Virgin Islands',\n",
    "        'VT': 'Vermont',\n",
    "        'WA': 'Washington',\n",
    "        'WI': 'Wisconsin',\n",
    "        'WV': 'West Virginia',\n",
    "        'WY': 'Wyoming'\n",
    "        }\n",
    "\n",
    "    all_jobs.State = all_jobs.State.replace(state_abvns, regex=True) \n",
    "    \n",
    "    all_jobs = all_jobs.drop('Location', axis=1)\n",
    "\n",
    "    return all_jobs\n",
    "\n",
    "#making this variable accessible from other functions\n",
    "all_jobs = find_all_jobs()\n",
    "\n",
    "#Let's see where the jobs are\n",
    "def where_are_jobs():\n",
    "    \n",
    "    job_locations = all_jobs.groupby('State').count()\n",
    "    job_locations = job_locations.sort_values('Job Title', ascending=False)\n",
    "\n",
    "    ax = job_locations['Job Title'].plot(kind='bar', title =\"Journalism Jobs by State\", figsize=(15, 5), legend=False, fontsize=12)\n",
    "    ax.set_xlabel(\"State\", fontsize=12)\n",
    "    ax.set_ylabel(\"Number of Jobs\", fontsize=12)\n",
    "    plt.show()\n",
    "    \n",
    "#find a job by location\n",
    "def jobs_in():\n",
    "    State = str(input('Enter state (sample input: Illinois):'))\n",
    "    all_jobs['State'].fillna(value=0).astype(str)\n",
    "    return  all_jobs.loc[all_jobs['State'].str.contains(State, na=False, case=False)]\n",
    "\n",
    "#find a job by keyword\n",
    "def search_jobtitles():\n",
    "    keyword = str(input('Enter keyword (sample input: editor):'))\n",
    "    all_jobs['Job Title'].fillna(value=0).astype(str)\n",
    "    return  all_jobs.loc[all_jobs['Job Title'].str.contains(keyword, na=False, case=False)]\n",
    "\n",
    "#open links to job postings from organization\n",
    "def see_job_site():\n",
    "    organization = str(input('Enter organization to see jobs on their website (sample input: marshall project):'))\n",
    "    all_jobs['Organization'].fillna(value=0).astype(str)\n",
    "    listed = all_jobs.loc[all_jobs['Organization'].str.contains(organization, na=False, case=False)]\n",
    "    for row in listed.Link:\n",
    "        row = row.strip()\n",
    "        webbrowser.open_new_tab(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
